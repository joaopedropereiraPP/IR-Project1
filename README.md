# IR-Project2

## The goal

Create a document indexer using the SPIMI approach and a basic searching mechanism. Three types of indexes are supported: raw (no term weighting), lnc.ltc and BM25. For all index types except raw there is also a query mechanism that return ranked results.

## Design

The application was completely developed in Python, using an object-oriented approach, to better guarantee encapsulation and modularity, with type annotations for all class members and methods to improve readability and unit tests for classes. It has the option of indexing with term positions, though this is disabled by default.

The SPIMI indexing limit per block that was implemented is based on number of postings, while the tokenizer implementation performs string preprocessing, stopword removal, word size filtering and stemming, which are all done in a single pass to prevent iterating each document multiple times.

It is prepared to parse Amazon review data files as the collection of documents to index, which follow the structure described on the beginning of this document: https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt. 

All index files are stored uncompressed as TSV files on the `index/data_source_name` subfolder, with the following structure:
* PostingIndex#.tsv - the final index files. Multiple files are created if the SPIMI posting limit per block is reached, where `'#'` is the block number. It contains the term on the first column of each row, and a posting on each subsequent column, as its document ID, which for weighted indexes is followed by the character `':'` and the posting weight and if positions are enabled by another`':'` and the list of positions on the document separated by `','`.
* TempBlock#.tsv - temporary index blocks used for merging into the final index. Multiple files are created if the SPIMI posting limit per block is reached, where `'#'` is the block number. The structure is the same as for the final index. Though it isn't necessary these blocks are kept after the final index is created so that they can be inspected.
* MasterIndex.tsv - Contains the document frequency (IDF if it's a weighted index type) and the final index block number where it can be found. The terms are on the first column of each row, followed on each column by its document frequency and the block number of the posting index.
* DocKeys.tsv - contains the correspondence of surrogate keys to natural keys, that is, the keys generated by the program and the original hexadecimal keys from Amazon, as well as the document title.

The BM25 weights are calculated for each posting already containing the multiplication by the inverted document frequency (IDF), in this manner it is intended to optimize the performance on the side of the queries while losing some performance while indexing, so that the program is more responsive on the side of the user.

The logarithms for the calculation of the weights and IDF are the most expensive operation among the indexing tasks, so this was optimized through a dynamic programming approach to resuse previous calculations with the same input for the logarithm, as the inputs are integer which are likely to be repeated. For the IDF the calculation was divided into two logarithms (log(N) - log(DF)), so that all logarithm calculations have an integer as input.

The base class for the index types is the raw index type, for which there are subclasses for the lnc.ltc index type and the BM25 index type. An hierarchy of posting classes was also created to abstract the structure of the index and how each posting is written to the disk and parsed back to memory for all index types and positional and nonpositional indexes.

## Prerequisites

The program was developed using Python 3.8, and in addition the PyStemmer module is required (https://github.com/snowballstem/pystemmer), which is a C implementation of snowballstemmer, and was chosen due to its improved performance compared to the default implementation of snowballstemmer.
Install it with the command:
```
pip install PyStemmer
```
## Usage
```
usage: main.py [-h] --mode indexer/searcher [--method raw/lnc.ltc/bm25] [--data_path path to data file (.gz))] [--nostopwords] [--stopwords (path to stopwords list)] [--word_size (integer number)] [--no_word_size] [--no_stemmer]
               [--use_positions] [--max_post MAX_POST] [--data DATA] [--search_type file (file-path)/loop [file (file-path/loop ...]] [--dump_file] [--cmd_results]

optional arguments:
  -h, --help            show this help message and exit
  --mode indexer/searcher
                        Set the main mode
  --method raw/lnc.ltc/bm25
                        Set the method
  --data_path (path to data file (.gz))
                        Set the path to the data, it should be relative to the program directory
  --nostopwords         Disable stop words
  --stopwords (path to stopwords list)
                        Set the path to stop words List
  --word_size (integer number)
                        Set the maximum for the word size filter
  --no_word_size        Disable word size filter
  --no_stemmer          Disable stemmer
  --use_positions       Enable positions indexing
  --max_post MAX_POST   Set the maximum postings per block
  --data DATA           Folder that contains the index files for query mode, it should be a folder inside the 'index' subfolder of the program
  --search_type file (file-path)/loop [file (file-path)/loop ...]
                        Choose the search mode, 'file (file-path)' to use a file with a list of queries as input, 'loop' to insert queries in a loop through the terminal (empty query to end loop)
  --dump_file           Enable to generate file with results
  --cmd_results         Enable to show the results on terminal
```

* The data_path option is the path to the Amazon review data file to be indexed.
* The nostopwords option disables stopwords.
* The stopwords option sets the path to the stopwords list, which should be a simple text file with a stop word on each row. The default is the file in `content/stopwords.txt`, which was taken from http://www.search-engines-book.com/data/stopwords.
* The word_size option sets the word size filter value, words with smaller or equal size than the value set here are filtered. The defaut is 3.
* The no_word_size option disables the word size filter.
* The no_stemmer option disables stemming.
* The use_positions option enables/disables term positions on the index, default is off.

After running the program the data file starts to be indexed using the SPIMI approach and the index files are created as described in the Design section. When it is done some statistics on the process are returned and the user is asked to enter the search term, for which the document frequency and final index file block number in which its postings are contained is returned, that is, the `#` in PostingIndex#.tsv, as described previously.

## Example

Using default parameters:
```
python3 src/main.py --data_path (path)
```

With new data file and stopwords disabled:
```
python3 src/main.py --data_path (path) --nostopwords
```

Indexer mode:
```
python3 src/main.py --mode indexer --method lnc.ltc --data_path content/amazon_reviews_us_Digital_Music_Purchase_v1_00.tsv.gz
```
Searcher mode with loop:
```
python3 src/main.py --mode searcher --data amazon_reviews_us_Digital_Music_Purchase_v1_00 --search_type loop --cmd_results
``` 
Searcher mode with query file and dumping results to file:
```
python3 src/main.py --mode searcher --data amazon_reviews_us_Digital_Music_Purchase_v1_00 --search_type file queries/queries.txt --dump_file
``` 

## The results

The sample amazon_reviews_us_Digital_Music_Purchase_v1_00.tsv.gz from https://s3.amazonaws.com/amazon-reviews-pds/readme.html was used.

For the lnc.ltc index type the results were the following:
Configuration:
* index_type	lnc.ltc
* size_filter	3
* stemmer_enabled	True
* stopwords_path	content/stopwords.txt
* use_positions	False
Statistics:
* Number of indexed documents: 1688884
* Number of postings: 33469012
* Vocabulary size: 271867
* Total indexing time (s): 478.55587339401245
* Total index size on disk (MB): 993.258436
* Number of temporary index segments: 34

For the BM25 index type the results were the following:
Configuration:
* index_type	bm25
* size_filter	3
* stemmer_enabled	True
* stopwords_path	content/stopwords.txt
* use_positions	False
Statistics:
* Number of indexed documents: 1688884
* Number of postings: 33469012
* Vocabulary size: 271867
* Total indexing time (s): 471.712206363678
* Total index size on disk (MB): 969.322838
* Number of temporary index segments: 34

The ranking results for each index type were tested for the list of queries on the file `queries/queries.txt` and the results for each are on the subfolder `results`.

These results were produced on a machine with the following specifications:
* CPU: AMD Ryzen 5 4600H with Radeon Graphics
* RAM: 8GB
* Storage: SSD - Samsung MZALQ512HALU-000L2

## Authors

Ivo Félix - 109641 [GitHub](https://github.com/IvoFelix)

João Pedro Pereira - 106346 [GitHub](https://github.com/joaopedropereiraPP)
